{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df6a159-0764-468f-baaa-cac1ba6e0161",
   "metadata": {
    "tags": []
   },
   "source": [
    "### This scraper functions as follows:\n",
    "1) With setup_driver, it launches the driver and sets it up\n",
    "<span style=\"color:red\">Would be interesting to already have the links ready for scraping\n",
    "considering you just have to add sth like \"\"\"?sort.sortType=RD&sort.ascending=false&filter.iso3Language=eng\"\"\"\n",
    "at the end.\n",
    "</span>\n",
    "2) With setup_url_site, it uses some filters for language and to have chronological order\n",
    "3) With single_page scraper, it scrapes all the required info on a single page\n",
    "4) With scrape_all_pages, it takes the link of the first page after filters (they affect the link)\n",
    "<span style=\"color:red\"> The link can be simply altered to go to the next page</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0567281-a783-4fec-abec-ee0532a753e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question\n",
    "- QUANTITY of pages (already 2900pages for pwc) ==> 1/5\n",
    "- Should I use link alteration or click? Behaviour human?\n",
    "- What is an acceptable rate of scraping (request/sec) so that I don't get banned?\n",
    "- Should I do most hiring HEC companies?\n",
    "- For how many years should I scrape?\n",
    "- Should I only scrape belgian salaries then? Bc I cannot do that for reviews. For different jobs maybe do dimensionality reduction. OR check latent semantic analysis.\n",
    "- I took only english is that okay?\n",
    "- Should I pull one df per company? And retrain each time? \n",
    "- Sentiment of pros and cons? what can I get out of that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc70b8-8860-4e7c-acc5-14a56b0d9e3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### IDEA: \n",
    "- Try scraping salaries and see outcome for most hiring companies of HEC\n",
    "- What about trying to cluster people by ratings etc, maybe I could reject the eternally unhappy (rageux) and the eternally happy (n'ont jamais vu autre chose) to just keep the relevant ones\n",
    "- Check relationship between sentence length or vocabulary diversity and score?\n",
    "- Check words that correlate the most with rating and do word clouds for every year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e2a13a9-4a1c-47b4-95f2-54e908f56e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import ElementClickInterceptedException, ElementNotInteractableException, WebDriverException\n",
    "from getting_link import getting_company_reviews_urls_bs\n",
    "\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c09da8-8616-4505-a70b-97da75858ea7",
   "metadata": {},
   "source": [
    "### Initializing variables and scraping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80c4086-0d2a-4abd-a815-8d575706fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializating variables\n",
    "subrating_classes = {'css-xd4dom': '*',\n",
    "               'css-18v8tui': '**',\n",
    "               'css-vl2edp': '***',\n",
    "               'css-1nuumx7': '****',\n",
    "               'css-s88v13': '*****'}\n",
    "shape_styles = {'css-10xv9lv-svg': 'O',\n",
    "         'css-1kiw93k-svg': 'X',\n",
    "         'css-1h93d4v-svg': '-',\n",
    "         'css-hcqxoa-svg': 'V'}\n",
    "filter_url = '.htm?sort.sortType=RD&sort.ascending=false&filter.iso3Language=eng&filter.employmentStatus=PART_TIME&filter.employmentStatus=REGULAR&filter.employmentStatus=INTERN'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bbcb59-0172-407f-95a2-1d244916d23e",
   "metadata": {},
   "source": [
    "### Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4c7fa32-38e4-48e3-9118-3c624ca126f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rid_overlay(driver):\n",
    "    \"\"\"\n",
    "    Sometimes the site shows an overlay that cannot be removed by clicking, which blocks interaction with the site,\n",
    "    this is some code to remove it.\n",
    "    \"\"\"\n",
    "    driver.execute_script(\"\"\"\n",
    "        javascript:(function(){\n",
    "          document.getElementsByClassName('hardsellOverlay')[0].remove();\n",
    "          document.getElementsByTagName(\"body\")[0].style.overflow = \"scroll\";\n",
    "          let style = document.createElement('style');\n",
    "          style.innerHTML = `\n",
    "            #LoginModal {\n",
    "              display: none!important;\n",
    "            }\n",
    "          `;\n",
    "          document.head.appendChild(style);\n",
    "          window.addEventListener(\"scroll\", function (event) {\n",
    "            event.stopPropagation();\n",
    "          }, true);\n",
    "        })();\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28fbbc06-4518-41f7-b205-b56b9b9a3535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rid_overlay(driver):\n",
    "    \"\"\"\n",
    "    Sometimes the site shows an overlay that cannot be removed by clicking, which blocks interaction with the site,\n",
    "    this is some code to remove it.\n",
    "    \"\"\"\n",
    "    driver.execute_script(\"\"\"\n",
    "        javascript:(function(){\n",
    "          document.getElementsByClassName('hardsellOverlay')[0].remove();\n",
    "          document.getElementsByTagName(\"body\")[0].style.overflow = \"scroll\";\n",
    "          let style = document.createElement('style');\n",
    "          style.innerHTML = `\n",
    "            #LoginModal {\n",
    "              display: none!important;\n",
    "            }\n",
    "          `;\n",
    "          document.head.appendChild(style);\n",
    "          window.addEventListener(\"scroll\", function (event) {\n",
    "            event.stopPropagation();\n",
    "          }, true);\n",
    "        })();\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d39ac77-2b78-44a2-a325-ef7e355d9e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "def setup_driver():\n",
    "    \"\"\"\n",
    "    Launches driver, maximizes window size\n",
    "    \"\"\"\n",
    "    driver = webdriver.Chrome('/Users/corentin/Documents/chromedriver_2')\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "    # Setting window size to make sure the scrolls get to where they are supposed to\n",
    "    driver.set_window_size(1920, 1080)\n",
    "    driver.implicitly_wait(20)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9674f60-800a-4574-9e68-af632db2307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_ratings(post):\n",
    "    \"\"\"\n",
    "    Returns a string with the main rating and a list with subrating\n",
    "    \"\"\"\n",
    "    # Main rating\n",
    "    main_rating = post.find('span', class_ = 'ratingNumber mr-xsm').string\n",
    "    \n",
    "    # Subratings\n",
    "    try:\n",
    "        rating_categories = post.find('div', class_ = 'content')\n",
    "        ratings = rating_categories.find_all('li') # Each rating has a different class, it's the only way to differentiate\n",
    "\n",
    "        subratings = {}\n",
    "        for subrating in ratings:\n",
    "            #Get categories, it's not always 6 of them\n",
    "            subrating_category = subrating.find_all('div')[0].string\n",
    "            subrating_class = subrating.find_all('div')[1]['class'][0]\n",
    "            subrating = subrating_classes[subrating_class]\n",
    "            subratings[subrating_category] = subrating\n",
    "    except:\n",
    "        subratings = {'Work/Life Balance': np.nan,\n",
    "                      'Culture & Values': np.nan, \n",
    "                      'Diversity & Inclusion': np.nan, \n",
    "                      'Career Opportunities': np.nan,\n",
    "                      'Compensation and Benefits': np.nan,\n",
    "                      'Senior Management': np.nan}\n",
    "    \n",
    "    return main_rating, subratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "379353fe-9566-4e59-8ac0-0df09d8ab805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_page_scraper(html): #Needs the driver.page_source\n",
    "    \"\"\"\n",
    "    Scrapes rating, subrating, post_title, date_and_job, recommend, CEO approval,\n",
    "    business outlook, pros and cons of all postings on a single page and\n",
    "    returns a dataframe\n",
    "    \"\"\"\n",
    "    #Initializing df\n",
    "    df = pd.DataFrame({'main_rating': [],\n",
    "                       'Work/Life Balance': [], \n",
    "                       'Culture & Values': [], \n",
    "                       'Diversity & Inclusion': [], \n",
    "                       'Career Opportunities': [],\n",
    "                       'Compensation and Benefits': [],\n",
    "                       'Senior Management': []\n",
    "                       })\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    # Get postings on a single page\n",
    "    # First and other nine reviews have different classes\n",
    "    first_posting = soup.find_all('li', class_ = 'empReview cf pb-0 mb-0')\n",
    "    next_postings = soup.find_all('li', class_ = 'noBorder empReview cf pb-0 mb-0')\n",
    "    # Extending first_posting with the nine other and renaming the variable adequatly\n",
    "    first_posting.extend(next_postings)\n",
    "    postings = first_posting\n",
    "    \n",
    "    \n",
    "    # Looping on postings\n",
    "    for post in postings:\n",
    "        # Get status\n",
    "        status = post.find('span', class_ = 'pt-xsm pt-md-0 css-1qxtz39 eg4psks0').string\n",
    "        # Get general opinion \n",
    "        post_title = post.find('h2', class_ = 'mb-xxsm mt-0 css-93svrw el6ke055').string\n",
    "        # Get date and job title\n",
    "        date_and_job = post.find('span', class_ = 'authorInfo').text        \n",
    "        # Get pros and cons\n",
    "        pros = post.find('span', {'data-test': 'pros'}).string\n",
    "        cons = post.find('span', {'data-test': 'cons'}).string\n",
    "        # Get shapes for Recommend, CEO approval, Business outlook for a single post\n",
    "        other_scores = {'Recommend': '',\n",
    "                       'CEO Approval': '',\n",
    "                       'Business Outlook': ''}\n",
    "        bar_shapes = post.find('div', class_ = 'd-flex my-std reviewBodyCell recommends css-1y3jl3a e1868oi10')\n",
    "        for i, element in  enumerate(bar_shapes.find_all('svg')):\n",
    "            other_scores[list(other_scores)[i]] = shape_styles[element['class'][1]]\n",
    "        #Initating row for dataframe \n",
    "        post_data = {'date_and_job': date_and_job,\n",
    "                     'post_title': post_title,\n",
    "                     'status': status,\n",
    "                     'pros': pros,\n",
    "                     'cons': cons,\n",
    "                     'Recommend': other_scores['Recommend'],\n",
    "                     'CEO Approval': other_scores['CEO Approval'],\n",
    "                     'Business Outlook': other_scores['Business Outlook']}\n",
    "        # Getting ratings\n",
    "        main_rating, subratings = scrape_ratings(post)\n",
    "        post_data['main_rating'] = main_rating\n",
    "        for key, value in subratings.items():\n",
    "            post_data[key] = value\n",
    "             \n",
    "        #Populating the dataframe\n",
    "        df = df.append((post_data),\n",
    "                        ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72f8792d-d43b-48e8-b43d-2e8cd2834047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_nofilter(url):\n",
    "    htm_index = url.index('.htm')\n",
    "    url_nofilter = url[:htm_index]\n",
    "    return url_nofilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11329d9e-2594-44ff-896d-c811eac62f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link(url, page):\n",
    "    nofilter = get_company_nofilter(url)\n",
    "    return nofilter + '_IP' + str(page) + filter_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27b39194-c81d-4e17-b465-96e0b2b8d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set site language to us\n",
    "def set_site_language(driver):\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "    language_choice = driver.find_element_by_xpath('//*[@id=\"Footer\"]/nav/ul[2]/li[3]/div/div/div[1]')\n",
    "    language_choice.click()\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    language_click_box = soup.find('div', class_='dropdownOptions dropdownExpanded animated above')\n",
    "    for li in language_click_box.find_all('li'):\n",
    "        country = li.find_all('span')[1].string\n",
    "        if country == 'United States':\n",
    "            us_id = li.get('id')\n",
    "            break\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "    us = driver.find_element_by_id(us_id)\n",
    "    us.click()\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9f1a495-403a-4e2b-927e-da846c53df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraped = pd.DataFrame({'company': [0],\n",
    "                           'belgian_reviews': [0], \n",
    "                           'lux_reviews': [0]}, index = range(len(df_links_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e531ff9-53c5-47c6-b2eb-216b435d34f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>belgian_reviews</th>\n",
       "      <th>lux_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   company  belgian_reviews  lux_reviews\n",
       "0        0                0            0\n",
       "1        0                0            0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef14de52-4587-4383-8d24-0d7aaad26e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews_count(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    review_count = soup.find('h2', {'data-test': 'overallReviewCount'}).find('strong').text\n",
    "    return review_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa33f810-e237-4d4b-9a80-dc9cf0e818a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_reviews(url, driver):\n",
    "    \"\"\"\n",
    "    Returns all the reviews for a single filtered company\n",
    "    Assumes to be on the first page of a company\n",
    "    \"\"\"\n",
    "    df_reviews = pd.DataFrame({'main_rating': [],\n",
    "                   'Work/Life Balance': [], \n",
    "                   'Culture & Values': [], \n",
    "                   'Diversity & Inclusion': [], \n",
    "                   'Career Opportunities': [],\n",
    "                   'Compensation and Benefits': [],\n",
    "                   'Senior Management': [],\n",
    "                   'Business Outlook': [],\n",
    "                   'CEO Approval': [],\n",
    "                   'Recommend': [],\n",
    "                   'cons': [],\n",
    "                   'date_and_job': [],\n",
    "                   'post_title': [],\n",
    "                   'pros': [],\n",
    "                   'status': []})\n",
    "    \n",
    "    # How many pages to scrape\n",
    "    reviews_count = int(get_reviews_count(driver.page_source))\n",
    "    if (reviews_count % 10 != 0):\n",
    "        pages = int(math.ceil(reviews_count / 10))\n",
    "    else:\n",
    "        pages = int(reviews_count / 10)\n",
    "    \n",
    "    # Loop on all pages\n",
    "    for page in range(1, pages+1):\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "        driver.get(get_link(url, page))\n",
    "        df_page = single_page_scraper(driver.page_source)\n",
    "        df_reviews = df_reviews.append(df_page, ignore_index=True)\n",
    "    \n",
    "    return df_reviews, driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dab5340b-25de-4ca6-bdfc-4513d5f57786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews_df(df_links):\n",
    "    \n",
    "    # Getting site in english, will stay so as long as driver isn't closed\n",
    "    driver = setup_driver()\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "    driver.get('https://www.glassdoor.com/Reviews/index.htm')\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "    driver = set_site_language(driver)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "    \n",
    "    # Go over dataframe and scrape all \n",
    "    for i, company in enumerate(df_links['company']):\n",
    "        # Loops over belgian and lux reviews\n",
    "            for column in df_links.columns[1:]:\n",
    "                    if isinstance(df_links[column].iloc[i], float):\n",
    "                        continue\n",
    "                    else:\n",
    "                        time.sleep(random.uniform(2, 4))\n",
    "                        driver.get(get_link(df_links[column].iloc[i], 1))\n",
    "                        time.sleep(random.uniform(2, 4))\n",
    "                        reviews, driver = get_all_reviews(driver.current_url, driver)\n",
    "                        reviews.to_csv(f'data/reviews/{company}_{column}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "044209b8-a506-4470-89dc-bd3fb4666ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_links = pd.read_csv('data/links_luxobelg.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e72c960-839d-42a2-8529-04647436bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_reviews_df(df_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "376a2294-a624-416e-ad64-36de7e69f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scrape_all_pages(driver, url): # Would like to be able to scrape 5 years back, you can try to use date in date_and_job column\n",
    "#     \"\"\"\n",
    "#     Takes link of first setted (english, recent) reviews, scrapes it and \n",
    "#     goes automatically to next pages scrapes them etc\n",
    "#     \"\"\"\n",
    "    \n",
    "#     df = pd.DataFrame()\n",
    "#     domain_page_url,  filter_url, company_code = url_decomposer(url)\n",
    "#     for p in range(3):\n",
    "#         try:\n",
    "#             driver.get(url)\n",
    "#             time.sleep(random.uniform(2, 4))\n",
    "#             df_p = single_page_scraper(driver.page_source)\n",
    "#             df = df.append(df_p, ignore_index=True)\n",
    "#             time.sleep(random.uniform(7, 13))\n",
    "#             url = domain_page_url + company_code + '_IP' + str(p) + filter_url\n",
    "#         except WebDriverException:\n",
    "#             get_rid_overlay(driver)\n",
    "#             time.sleep(random.uniform(2, 4))\n",
    "#             df_p = single_page_scraper(driver.page_source)\n",
    "#             df = df.append(df_p, ignore_index=True)\n",
    "#             time.sleep(random.uniform(7, 13))\n",
    "#             url = domain_page_url + company_code + '_IP' + str(p) + filter_url\n",
    "        \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "301ea79c-1397-4740-ba82-377ff08750ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def url_decomposer(url):\n",
    "#     \"\"\"\n",
    "#     Takes any url, and returns the domain, the filter part of the url and the company code part of the url, \n",
    "#     with these you can then recompose the link of any page of that company's reviews as such:\n",
    "#     domain_page_url + company_code + _IPx + filter_url with x the page in question\n",
    "#     \"\"\"\n",
    "#     domain_page_url = 'https://www.glassdoor.com/Reviews/'\n",
    "    \n",
    "#     filter_index = url.index('.htm')\n",
    "#     filter_url = url[filter_index:]\n",
    "    \n",
    "#     try:\n",
    "#         page_index = url.index('_IP')\n",
    "#         page_url = url[page_index:]\n",
    "#     except ValueError:\n",
    "#         page_url = ''\n",
    "    \n",
    "#     company_code = url.replace(page_url, '').replace(domain_page_url, '').replace(filter_url, '')\n",
    "#     return domain_page_url,  filter_url, company_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9b71d964-804a-4cea-bf61-10d0abea3fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only english reviews\n",
    "# def get_english_reviews():\n",
    "#     time.sleep(random.uniform(2, 4))\n",
    "#     filter_button = driver.find_element_by_xpath('//*[@id=\"MainContent\"]/div/div[1]/div[1]/div[1]/div/div[1]/div[2]/div[2]/button/span')\n",
    "#     filter_button.click()\n",
    "#     time.sleep(random.uniform(2, 4))\n",
    "#     language_button = driver.find_element_by_xpath('//*[@id=\"MainContent\"]/div/div[1]/div[1]/div[1]/div/div[1]/div[3]/div[2]/div[2]/div/div[1]')\n",
    "#     language_button.click()\n",
    "#     time.sleep(random.uniform(2, 4))\n",
    "#     english = driver.find_element_by_xpath('//*[@id=\"option_eng\"]')\n",
    "#     english.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9aa59af3-5da3-4e84-8b92-e2e4a1aa654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with most recent reviews\n",
    "# def orderby_recent():\n",
    "#     time.sleep(random.uniform(2, 4))\n",
    "#     driver.execute_script('window.scrollTo(0, 450)')\n",
    "#     time.sleep(random.uniform(2, 4))\n",
    "#     freshness_button = driver.find_element_by_xpath('//*[@id=\"MainContent\"]/div/div[1]/div[1]/div[2]/div/span/div/div/div[1]')\n",
    "#     freshness_button.click()\n",
    "#     time.sleep(random.uniform(2, 4))\n",
    "#     most_recent = driver.find_element_by_xpath('//*[@id=\"option_DATE\"]')\n",
    "#     most_recent.click()\n",
    "#     time.sleep(random.uniform(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "caca307b-f3bd-45a7-ab1a-0f84b60d311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only belgian reviews\n",
    "# def get_belgian_reviews():\n",
    "#     time.sleep(random.uniform(2, 4))\n",
    "#     filter_button = driver.find_element_by_xpath('//*[@id=\"MainContent\"]/div/div[1]/div[1]/div[1]/div/div[1]/div[2]/div[2]/button/span')\n",
    "#     filter_button.click()\n",
    "#     time.sleep(random.uniform(2, 4))\n",
    "#     location_box = driver.find_element_by_xpath('//*[@id=\"MainContent\"]/div/div[1]/div[1]/div[1]/div/div[1]/div[3]/div[2]/div[1]/div/div[1]')\n",
    "#     location_box.click()\n",
    "#     time.sleep(random.uniform(2, 4))\n",
    "#     location_input = driver.find_element_by_xpath('//*[@id=\"MainContent\"]/div/div[1]/div[1]/div[1]/div/div[1]/div[3]/div[2]/div[1]/div/div[1]/div/div/div/input')\n",
    "#     location_input.send_keys('Belgium')\n",
    "#     belgium = driver.find_element_by_xpath('//*[@id=\"option_N,25\"]/span')\n",
    "#     belgium.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "67da0981-5773-444f-a599-bc22f3a8af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Interns and full time and part time\n",
    "# def interns_and_fulltime():\n",
    "#     time.sleep(random.uniform(2, 4))\n",
    "#     filter_button = driver.find_element_by_xpath('//*[@id=\"MainContent\"]/div/div[1]/div[1]/div[1]/div/div[1]/div[2]/div[2]/button/span')\n",
    "#     filter_button.click()\n",
    "#     time.sleep(random.uniform(2, 4))\n",
    "#     jobs_category = driver.find_element_by_xpath('//*[@id=\"MainContent\"]/div/div[1]/div[1]/div[1]/div/div[1]/div[3]/div[1]/div[2]/div/div[1]')\n",
    "#     jobs_category.click()\n",
    "#     time.sleep(random.uniform(2, 4))\n",
    "#     jobs_categories = driver.find_element_by_xpath('//*[@id=\"option_INTERN\"]/div')\n",
    "#     jobs_categories.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "93ccadcf-e1b9-4484-9e98-510a2412157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def setup_url_site(driver, url):\n",
    "#     \"\"\"\n",
    "#     Gets the url, sets the site in english, set the reviews in english, sets the \n",
    "#     reviews to chronological order, gets only belgian reviews for intern, full time and part time jobs.\n",
    "#     \"\"\"\n",
    "#     driver.get(url)\n",
    "#     functions = [set_site_language, get_english_reviews, orderby_recent, get_belgian_reviews, interns_and_fulltime]\n",
    "    \n",
    "#     for func in functions:\n",
    "#         max_tries = 5\n",
    "#         tries = 0\n",
    "#         if tries <= max_tries:\n",
    "#             try:\n",
    "#                 func()\n",
    "#             except (ElementClickInterceptedException, ElementNotInteractableException, WebDriverException):\n",
    "#                 get_rid_overlay(driver)\n",
    "#                 tries +=1\n",
    "#                 func()\n",
    "#         else:\n",
    "#             print('Max tries for setup attained')\n",
    "    \n",
    "#     # Returns modified url with all filters\n",
    "#     url = driver.current_url\n",
    "#     return driver, url"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
